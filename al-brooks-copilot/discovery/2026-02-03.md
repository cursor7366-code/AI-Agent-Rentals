# Discovery Report ‚Äî 2026-02-03

## Summary

- **New discoveries**: 10
- **HIGH priority**: 3
- **MEDIUM priority**: 4
- **WATCHING**: 3
- **Sources checked**: GitHub (time series, RAG, trading, structured output)

---

## üî¥ HIGH PRIORITY ‚Äî Evaluate This Week

### 1. Google TimesFM
- **Link**: https://github.com/google-research/timesfm
- **Stars**: 7,700 ‚≠ê | Last updated: Today
- **Category**: Time Series / Regime Detection

**What it does**: Google's pre-trained foundation model for time series forecasting. Zero-shot or fine-tunable.

**How it helps YOUR project**: Your regime inference step needs to assess "are we trending, ranging, or breaking out?" TimesFM could provide probabilistic forecasts WITHOUT training your own model. Feed it your OHLC bars, get regime probabilities back. Solves the "regime_inference.py" bottleneck directly.

**Integration effort**: Medium (1-2 days). Python package, well-documented, can run inference locally.

**Why HIGH**: Directly addresses your Step 3 (Regime Inference). Mature, Google-backed, active development.

---

### 2. Instructor
- **Link**: https://github.com/567-labs/instructor
- **Stars**: 12,300 ‚≠ê | Last updated: Today
- **Category**: Structured Output / LLM

**What it does**: Guarantees structured JSON/Pydantic output from any LLM. No more parsing failures.

**How it helps YOUR project**: Your RegimeAssessment and Decision outputs need to be reliable structured data (not freeform text that might fail to parse). Instructor wraps your LLM calls and ensures you ALWAYS get valid `RegimeHypothesis`, `Decision`, etc. objects back.

**Integration effort**: Low (hours). Drop-in replacement for OpenAI/Anthropic calls. Works with your existing Pydantic dataclasses.

**Why HIGH**: You're already using structured outputs. This makes them bulletproof. Prevents the "LLM returned garbage" failure mode in live trading.

---

### 3. LightRAG
- **Link**: https://github.com/HKUDS/LightRAG
- **Stars**: 27,800 ‚≠ê | Last updated: Today
- **Category**: Retrieval

**What it does**: Fast, lightweight RAG with graph-based retrieval. 10x faster than GraphRAG with similar quality.

**How it helps YOUR project**: Your Layer 3 (Semantic Search) needs to be FAST for live trading. LightRAG is specifically optimized for speed while maintaining retrieval quality. Could replace or augment your current LanceDB setup for the "get situation-specific chunks" step.

**Integration effort**: Medium (1-2 days). Python, good docs, can work alongside existing vector DB.

**Why HIGH**: Speed is critical for live trading. Your semantic search is "ALWAYS RUN" per your architecture ‚Äî this makes it faster.

---

## üü° MEDIUM PRIORITY ‚Äî Research Queue

### 4. Time-MoE
- **Link**: https://github.com/Time-MoE/Time-MoE
- **Stars**: 898 ‚≠ê | Last updated: Today
- **Category**: Time Series Foundation Model

**What it does**: Billion-parameter time series model using Mixture of Experts. ICLR 2025 Spotlight paper.

**How it helps YOUR project**: More powerful than TimesFM for complex regime detection. The MoE architecture means different "experts" activate for different market conditions (trending vs ranging) ‚Äî aligns perfectly with your multi-hypothesis approach.

**Why MEDIUM**: Newer than TimesFM, less battle-tested. Evaluate TimesFM first, keep this as upgrade path.

**Open questions**: Inference speed? Memory requirements? Fine-tuning difficulty?

---

### 5. Lag-Llama
- **Link**: https://github.com/time-series-foundation-models/lag-llama
- **Stars**: 1,540 ‚≠ê | Last updated: 2 days ago
- **Category**: Probabilistic Forecasting

**What it does**: Foundation model for PROBABILISTIC time series forecasting. Outputs distributions, not point estimates.

**How it helps YOUR project**: Your regime inference needs PROBABILITIES ("60% trending, 30% ranging, 10% breakout"). Lag-Llama is designed exactly for this ‚Äî it gives you confidence intervals and probability distributions, not just "price will go up."

**Why MEDIUM**: Great fit conceptually, but need to verify it works at 5-minute bar scale (most time series models trained on daily+ data).

---

### 6. Rankify
- **Link**: https://github.com/DataScienceUIBK/Rankify
- **Stars**: 581 ‚≠ê | Last updated: Today
- **Category**: Retrieval + Reranking

**What it does**: Comprehensive toolkit with 7 retrieval methods + 24 reranking models in one package.

**How it helps YOUR project**: Your retrieval quality problem ("getting the RIGHT chunks") could benefit from reranking. First-pass retrieval gets candidates, reranker picks the truly relevant ones. This gives you 24 rerankers to test without implementing each.

**Why MEDIUM**: Your retrieval works, this is an optimization. But could significantly improve "situation-specific context" quality.

---

### 7. BCEmbedding
- **Link**: https://github.com/netease-youdao/BCEmbedding
- **Stars**: 1,860 ‚≠ê | Last updated: 4 days ago
- **Category**: Embeddings + Reranker

**What it does**: Production-grade embedding + reranker models specifically designed for RAG.

**How it helps YOUR project**: Could improve your embedding quality for the 300 pattern summaries and chunk indexing. Better embeddings = better retrieval = better regime inference.

**Why MEDIUM**: Your embeddings work. This is an upgrade path, not a fix for a broken thing.

---

## üëÄ WATCHING ‚Äî Check Back Later

### 8. TimeCopilot
- **Link**: https://github.com/TimeCopilot/timecopilot
- **Stars**: 364 ‚≠ê | Last updated: Today
- **Category**: Forecasting Agent

**What it does**: GenAI forecasting agent that wraps multiple time series foundation models in one API.

**How it helps YOUR project**: Instead of picking ONE time series model, this lets you query multiple and compare. "Ask 3 models if we're trending" and aggregate answers.

**Why WATCHING**: Young project (364 stars), but the multi-model approach is interesting. Check back in 2-3 weeks to see if it matures.

**Check back**: 2026-02-20

---

### 9. QuantDinger
- **Link**: https://github.com/brokermr810/QuantDinger
- **Stars**: 665 ‚≠ê | Last updated: Today
- **Category**: Trading Platform

**What it does**: AI-driven, local-first quantitative trading platform. Research, backtesting, live execution.

**How it helps YOUR project**: Full platform that might replace or complement your custom infrastructure. If it's good, could save you building the live execution layer.

**Why WATCHING**: Need to evaluate if it's actually good or just hype. "AI-driven" is vague. Check their architecture.

**Check back**: 2026-02-15

---

### 10. AI-Hedge-Fund-Crypto
- **Link**: https://github.com/51bitquant/ai-hedge-fund-crypto
- **Stars**: 506 ‚≠ê | Last updated: Yesterday
- **Category**: Trading + LLM Agents

**What it does**: LLM agents for trading decision-making. Multi-agent system for research ‚Üí decision ‚Üí execution.

**How it helps YOUR project**: Could provide patterns for how to structure your decision engine with LLM agents. Inspiration, not direct integration.

**Why WATCHING**: Crypto-focused (you're futures), but the agent architecture might be adaptable. Look at their code for ideas.

**Check back**: 2026-02-15

---

## Observations

1. **Time series foundation models are maturing fast.** Google TimesFM, Time-MoE, Lag-Llama ‚Äî there are now real options for regime detection without training from scratch.

2. **RAG is converging on graph + vector hybrid.** LightRAG, GraphRAG both use graphs to improve retrieval. Your hierarchical summaries are a form of this.

3. **Structured output is a solved problem.** Instructor has 12k stars for a reason. No excuse for LLM parsing failures anymore.

4. **Most "trading AI" repos are garbage.** Searched candlestick pattern detection ‚Äî all under 15 stars, abandoned. Your custom approach is necessary; there's no off-the-shelf solution.

---

## Recommended Action

**This week**: 
1. Try **Instructor** (easiest, biggest reliability win)
2. Evaluate **TimesFM** for regime inference prototype
3. Benchmark **LightRAG** speed vs your current LanceDB

**Next week**:
- If TimesFM works, compare against Lag-Llama for probabilistic outputs
- If retrieval still slow, try Rankify reranking
